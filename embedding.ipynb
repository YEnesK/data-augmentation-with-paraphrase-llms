{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOfGk+CHHywz7i3crZYAAIc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uWAiU1OO6mbL","executionInfo":{"status":"ok","timestamp":1736776348955,"user_tz":-180,"elapsed":21179,"user":{"displayName":"Yusuf Enes Kurt","userId":"14520496146386168194"}},"outputId":"d8a8d6ab-0f01-446e-a2e3-93bf00024658"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","directory = '/content/drive/MyDrive/Colab Notebooks/Kollektif_Ogrenme/Proje'"]},{"cell_type":"code","source":["!pip install datasets"],"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"BjUsDc2ABh8d","executionInfo":{"status":"ok","timestamp":1736776363685,"user_tz":-180,"elapsed":14732,"user":{"displayName":"Yusuf Enes Kurt","userId":"14520496146386168194"}},"outputId":"02758390-0e05-494b-ac56-899dc56f2493"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.10.0\n","    Uninstalling fsspec-2024.10.0:\n","      Successfully uninstalled fsspec-2024.10.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"]}]},{"cell_type":"code","source":["# Dosya isimlerini ve yolları belirleyin !!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","\n","imdb_train_embedding_filepath = directory+\"/imdb_dataset/train_imdb_5x_embeddings.pkl\"\n","imdb_test_embedding_filepath = directory+\"/imdb_dataset/test_imdb_6x_embeddings.pkl\"\n","\n","news_train_embedding_filepath = directory+\"/news_dataset/train_news_5x_embeddings.pkl\"\n","news_test_embedding_filepath = directory+\"/news_dataset/test_news_6x_embeddings.pkl\"\n","\n","# Dataset isim ve directory belirle !!!!!!!!!!!!!!!!!!!!!!\n","\n","imdb_train_dataset_filepath = directory+\"/imdb_dataset/train_imdb_5x.csv\"\n","imdb_test_dataset_filepath = directory+\"/imdb_dataset/test_imdb_6x.csv\"\n","\n","news_train_dataset_filepath = directory+\"/news_dataset/train_news_5x.csv\"\n","news_test_dataset_filepath = directory+\"/news_dataset/test_news_6x.csv\""],"metadata":{"id":"y3LfJBEC8MN8","executionInfo":{"status":"ok","timestamp":1736776714453,"user_tz":-180,"elapsed":264,"user":{"displayName":"Yusuf Enes Kurt","userId":"14520496146386168194"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["from datasets import Dataset, DatasetDict\n","import pandas as pd\n","\n","directory = '/content/drive/MyDrive/Colab Notebooks/Kollektif_Ogrenme/Proje'\n","\n","# CSV dosyalarından veriyi pandas DataFrame'lerine aktar\n","train_balanced = pd.read_csv(imdb_train_dataset_filepath)\n","test_balanced = pd.read_csv(imdb_test_dataset_filepath)\n","\n","# 5) CSV dosyalarından tekrar okuyarak Hugging Face Dataset'e dönüştürme\n","train_csv = pd.read_csv(news_train_dataset_filepath)\n","test_csv = pd.read_csv(news_test_dataset_filepath)\n","\n","\n","\n","# Pandas DataFrame'lerini Hugging Face Dataset nesnelerine dönüştür\n","train_ds = Dataset.from_pandas(train_balanced)\n","test_ds = Dataset.from_pandas(test_balanced)\n","\n","# Tek bir DatasetDict nesnesinde topla\n","ds = DatasetDict({\n","    \"train\": train_ds,\n","    \"test\": test_ds\n","})\n","\n","# Veri setini kontrol edelim\n","print(ds)\n","\n","train_dataset = Dataset.from_pandas(train_csv)\n","test_dataset = Dataset.from_pandas(test_csv)\n","\n","ds2 = DatasetDict({\n","    \"train\": train_dataset,\n","    \"test\": test_dataset\n","})\n","\n","print(ds2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fPwI5r6q64E7","executionInfo":{"status":"ok","timestamp":1736776718088,"user_tz":-180,"elapsed":1815,"user":{"displayName":"Yusuf Enes Kurt","userId":"14520496146386168194"}},"outputId":"7a705591-ac5d-45bc-d43b-cb83138d2ae8"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 10000\n","    })\n","    test: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 3000\n","    })\n","})\n","DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 10000\n","    })\n","    test: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 3000\n","    })\n","})\n"]}]},{"cell_type":"code","source":["from tqdm import tqdm\n","import torch\n","import pickle\n","import numpy as np\n","from transformers import AutoTokenizer, AutoModel  # [[2],[3]]\n","# ds_final veri kümesini (train ve test) bir önceki adımlarda oluşturduğunuzu varsayıyoruz.\n","# ds_final[\"train\"] -> eğitim\n","# ds_final[\"test\"]  -> test\n","\n","# Load model directly [[2],[3]]\n","tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")  # [[2]]\n","model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")         # [[2]]\n","model.eval()\n","\n","# GPU kullanımını tercih edin (varsa), performans açısından önemlidir [[3]]\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","def mean_pooling(model_output, attention_mask):\n","    \"\"\"\n","    Mean Pooling - take attention mask into account for correct averaging.\n","    \"\"\"\n","    token_embeddings = model_output.last_hidden_state  # [[7]]\n","    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","    # Sum token embeddings, then divide by sum of attention mask\n","    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","\n","def embed_texts(text_list):\n","    \"\"\"\n","    Verilen metin listesini tokenize edip AutoModel ile çıkarım yapar,\n","    son katmandan mean pooling ile ortalama embedding döndürür [[7]].\n","    \"\"\"\n","    embeddings = []\n","    with torch.no_grad():\n","        for text in tqdm(text_list):\n","            inputs = tokenizer(\n","                text,\n","                return_tensors=\"pt\",\n","                padding=True,\n","                truncation=True,\n","                max_length=512\n","            ).to(device)\n","            outputs = model(**inputs)\n","            pooled_embedding = mean_pooling(outputs, inputs['attention_mask'])\n","            # pooled_embedding shape: (batch_size=1, hidden_size)\n","            embeddings.append(pooled_embedding.squeeze(0).cpu().numpy())\n","    return embeddings\n","\n","\n","\n","\n","\n","# 1) imdb train datasetini göm\n","train_texts = ds[\"train\"][\"text\"]\n","train_embeddings = embed_texts(train_texts)\n","\n","with open(imdb_train_embedding_filepath, \"wb\") as f:\n","    pickle.dump(train_embeddings, f)\n","print(\"Kayıt tamamlandı: \"+imdb_train_embedding_filepath)\n","\n","\"\"\"\n","\n","# 2) imdb test datasetini göm\n","test_texts = ds[\"test\"][\"text\"]\n","test_embeddings = embed_texts(test_texts)\n","\n","with open(imdb_test_embedding_filepath, \"wb\") as f:\n","    pickle.dump(test_embeddings, f)\n","print(\"Kayıt tamamlandı: \"+imdb_test_embedding_filepath)\n","\n","\"\"\"\n","\n","\n","\n","\n","\n","\n","\n","# 1) news train datasetini göm\n","train_texts = ds2[\"train\"][\"text\"]\n","train_embeddings = embed_texts(train_texts)\n","\n","with open(news_train_embedding_filepath, \"wb\") as f:\n","    pickle.dump(train_embeddings, f)\n","print(\"Kayıt tamamlandı: \"+news_train_embedding_filepath)\n","\n","\n","\"\"\"\n","\n","# 2) news test datasetini göm\n","test_texts = ds2[\"test\"][\"text\"]\n","test_embeddings = embed_texts(test_texts)\n","\n","with open(news_test_embedding_filepath, \"wb\") as f:\n","    pickle.dump(test_embeddings, f)\n","print(\"Kayıt tamamlandı: \"+news_test_embedding_filepath)\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"id":"50goqFSP6pRT","executionInfo":{"status":"ok","timestamp":1736776836476,"user_tz":-180,"elapsed":113259,"user":{"displayName":"Yusuf Enes Kurt","userId":"14520496146386168194"}},"outputId":"8b501454-9959-4d67-cd96-cdc233f46cfc"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10000/10000 [00:58<00:00, 172.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Kayıt tamamlandı: /content/drive/MyDrive/Colab Notebooks/Kollektif_Ogrenme/Proje/imdb_dataset/train_imdb_5x_embeddings.pkl\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10000/10000 [00:54<00:00, 183.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Kayıt tamamlandı: /content/drive/MyDrive/Colab Notebooks/Kollektif_Ogrenme/Proje/news_dataset/train_news_5x_embeddings.pkl\n"]},{"output_type":"execute_result","data":{"text/plain":["'\\n\\n# 2) news test datasetini göm\\ntest_texts = ds2[\"test\"][\"text\"]\\ntest_embeddings = embed_texts(test_texts)\\n\\nwith open(news_test_embedding_filepath, \"wb\") as f:\\n    pickle.dump(test_embeddings, f)\\nprint(\"Kayıt tamamlandı: \"+news_test_embedding_filepath)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]}]}