{"cells":[{"cell_type":"markdown","metadata":{"id":"bdiD3qvghSzm"},"source":["## stanfordnlp/imdb"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HNULOTOqsKTp","outputId":"4d9fad75-45a3-478e-b178-f206ba053ab3","executionInfo":{"status":"ok","timestamp":1736592716521,"user_tz":-180,"elapsed":16786,"user":{"displayName":"Yusuf Enes Kurt","userId":"14520496146386168194"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","directory = '/content/drive/MyDrive/Colab Notebooks/Kollektif_Ogrenme/Proje'"]},{"cell_type":"code","execution_count":2,"metadata":{"collapsed":true,"id":"4OpwteSvltfo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736592723797,"user_tz":-180,"elapsed":7279,"user":{"displayName":"Yusuf Enes Kurt","userId":"14520496146386168194"}},"outputId":"f940b644-166a-461f-aa12-5c15a54c231e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.10.0\n","    Uninstalling fsspec-2024.10.0:\n","      Successfully uninstalled fsspec-2024.10.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"]}],"source":["!pip install datasets"]},{"cell_type":"code","source":["# Dataset isim ve directory belirle !!!!!!!!!!!!!!!!!!!!!!\n","\n","imdb_train_dataset_filepath = directory+\"/imdb_dataset/train_imdb.csv\"\n","imdb_test_dataset_filepath = directory+\"/imdb_dataset/test_imdb.csv\"\n","\n","news_train_dataset_filepath = directory+\"/news_dataset/train_news.csv\"\n","news_test_dataset_filepath = directory+\"/news_dataset/test_news.csv\"\n","\n","total_dataset = 2500"],"metadata":{"id":"6mubvL7v9LDj","executionInfo":{"status":"ok","timestamp":1736596329283,"user_tz":-180,"elapsed":203,"user":{"displayName":"Yusuf Enes Kurt","userId":"14520496146386168194"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3284,"status":"ok","timestamp":1736596334144,"user":{"displayName":"Yusuf Enes Kurt","userId":"14520496146386168194"},"user_tz":-180},"id":"U3bgHNSNfdro","outputId":"ec072f94-8756-47e9-92a3-40b1d81e1d8e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Label classes: ['neg', 'pos']\n","Label 'neg' count: 12500\n","Label 'pos' count: 12500\n","Train split row count: 2500\n","Train shape: (2000, 2)\n","Test shape: (500, 2)\n","\n","Eğitim kümesindeki label dağılımı:\n","label\n","1    1000\n","0    1000\n","Name: count, dtype: int64\n","\n","Test kümesindeki label dağılımı:\n","label\n","0    250\n","1    250\n","Name: count, dtype: int64\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-20-6047e4905bfb>:34: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  balanced_df = full_df.groupby('label').apply(lambda x: x.sample(samples_per_label, random_state=42)).reset_index(drop=True)\n"]}],"source":["from datasets import load_dataset\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Veri kümesini yükle\n","ds = load_dataset(\"stanfordnlp/imdb\")\n","\n","# 2) Label (etiket) sınıflarını elde et\n","label_names = ds[\"train\"].features[\"label\"].names\n","print(\"Label classes:\", label_names)\n","\n","# 3) Her bir label sınıfının sayısını yazdır\n","for label_id, label_name in enumerate(label_names):\n","    label_count = ds[\"train\"][\"label\"].count(label_id)\n","    print(f\"Label '{label_name}' count:\", label_count)\n","\n","# Train ve test splitlerini DataFrame'e çevir\n","train_df = pd.DataFrame(ds[\"train\"])\n","test_df = pd.DataFrame(ds[\"test\"])\n","\n","# Train ve test verilerini birleştir\n","full_df = pd.concat([train_df, test_df], ignore_index=True)\n","\n","# Her bir label sınıfından eşit sayıda örnek al\n","# Toplam total_dataset satır istiyoruz, bu yüzden her label'dan total_dataset / label_sayısı kadar örnek alacağız\n","label_counts = full_df['label'].value_counts()\n","min_count = label_counts.min()  # En az sayıda olan label sayısı\n","samples_per_label = total_dataset // len(label_counts)  # Her label'dan alınacak örnek sayısı\n","\n","# Eğer her label'dan daha fazla örnek almak mümkünse, o zaman bu sayıyı kullan\n","samples_per_label = min(samples_per_label, min_count)\n","\n","# Eşit dağılım için örnek al\n","balanced_df = full_df.groupby('label').apply(lambda x: x.sample(samples_per_label, random_state=42)).reset_index(drop=True)\n","\n","# Toplam satır sayısı (train split için)\n","print(\"Train split row count:\", len(balanced_df))\n","\n","# Veri setinde text ve label sütunları mevcuttur.\n","# %80 train, %20 test olacak şekilde dengeli ayır\n","train_balanced, test_balanced = train_test_split(\n","    balanced_df,\n","    test_size=0.2,\n","    stratify=balanced_df[\"label\"],\n","    random_state=42\n",")\n","\n","print(\"Train shape:\", train_balanced.shape)\n","print(\"Test shape:\", test_balanced.shape)\n","\n","print(\"\\nEğitim kümesindeki label dağılımı:\")\n","print(train_balanced[\"label\"].value_counts())\n","\n","print(\"\\nTest kümesindeki label dağılımı:\")\n","print(test_balanced[\"label\"].value_counts())\n","\n","# Ayrılmış veriyi CSV formatında kaydet\n","train_balanced.to_csv(imdb_train_dataset_filepath, index=False)\n","test_balanced.to_csv(imdb_test_dataset_filepath, index=False)"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1736596334145,"user":{"displayName":"Yusuf Enes Kurt","userId":"14520496146386168194"},"user_tz":-180},"id":"FXP6a7Z7tVcp","outputId":"42fde177-9c14-40f8-8ca0-8cf7160ac6d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 2000\n","    })\n","    test: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 500\n","    })\n","})\n"]}],"source":["from datasets import Dataset, DatasetDict\n","import pandas as pd\n","\n","directory = '/content/drive/MyDrive/Colab Notebooks/Kollektif_Ogrenme/Proje'\n","\n","# CSV dosyalarından veriyi pandas DataFrame'lerine aktar\n","train_balanced = pd.read_csv(directory+\"/imdb_dataset/train_imdb.csv\")\n","test_balanced = pd.read_csv(directory+\"/imdb_dataset/test_imdb.csv\")\n","\n","# Pandas DataFrame'lerini Hugging Face Dataset nesnelerine dönüştür\n","train_ds = Dataset.from_pandas(train_balanced)\n","test_ds = Dataset.from_pandas(test_balanced)\n","\n","# Tek bir DatasetDict nesnesinde topla\n","ds = DatasetDict({\n","    \"train\": train_ds,\n","    \"test\": test_ds\n","})\n","\n","# Veri setini kontrol edelim\n","print(ds)"]},{"cell_type":"markdown","metadata":{"id":"en5V2eX2hZWt"},"source":["## fancyzhx/ag_news"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"sXPIafPBvGho","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736592748514,"user_tz":-180,"elapsed":1585,"user":{"displayName":"Yusuf Enes Kurt","userId":"14520496146386168194"}},"outputId":"9b81ee32-18c0-4401-cbaf-252cd3eef248"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","directory = '/content/drive/MyDrive/Colab Notebooks/Kollektif_Ogrenme/Proje'"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4273,"status":"ok","timestamp":1736596341087,"user":{"displayName":"Yusuf Enes Kurt","userId":"14520496146386168194"},"user_tz":-180},"id":"3cuvzXG6haeW","outputId":"199df1f8-5182-441c-8f76-3931540bd1cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Label classes: ['World', 'Sports', 'Business', 'Sci/Tech']\n","Train split row count: 2500\n","Eğitim kümesi boyutu: (2000, 2)\n","Test kümesi boyutu: (500, 2)\n","\n","Eğitim kümesindeki sınıfların dağılımı:\n","label\n","1    500\n","2    500\n","0    500\n","3    500\n","Name: count, dtype: int64\n","\n","Test kümesindeki sınıfların dağılımı:\n","label\n","3    125\n","2    125\n","0    125\n","1    125\n","Name: count, dtype: int64\n","\n","Veriler 'train_news.csv' ve 'test_news.csv' olarak kaydedildi.\n","\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-22-56519d6e34c1>:30: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  balanced_df = full_df.groupby('label').apply(lambda x: x.sample(samples_per_label, random_state=42)).reset_index(drop=True)\n"]}],"source":["from datasets import load_dataset\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from datasets import Dataset, DatasetDict\n","\n","# 1) Veri kümesini yükle\n","ds2 = load_dataset(\"fancyzhx/ag_news\")\n","\n","# 2) Label (etiket) sınıflarını elde et\n","label_names = ds2[\"train\"].features[\"label\"].names\n","print(\"Label classes:\", label_names)\n","\n","# ds içindeki train ve test split'lerini pandas DataFrame'e çevir\n","train_df = pd.DataFrame(ds2[\"train\"])\n","test_df = pd.DataFrame(ds2[\"test\"])\n","\n","# 2) İki split'i birleştir\n","full_df = pd.concat([train_df, test_df], ignore_index=True)\n","\n","# Her bir label sınıfından eşit sayıda örnek al\n","# Toplam total_dataset satır istiyoruz, bu yüzden her label'dan total_dataset / label_sayısı kadar örnek alacağız\n","label_counts = full_df['label'].value_counts()\n","min_count = label_counts.min()  # En az sayıda olan label sayısı\n","samples_per_label = total_dataset // len(label_counts)  # Her label'dan alınacak örnek sayısı\n","\n","# Eğer her label'dan daha fazla örnek almak mümkünse, o zaman bu sayıyı kullan\n","samples_per_label = min(samples_per_label, min_count)\n","\n","# Eşit dağılım için örnek al\n","balanced_df = full_df.groupby('label').apply(lambda x: x.sample(samples_per_label, random_state=42)).reset_index(drop=True)\n","\n","# Toplam satır sayısı (train split için)\n","print(\"Train split row count:\", len(balanced_df))\n","\n","# Veri setinde text ve label sütunları mevcuttur.\n","# %80 train, %20 test olacak şekilde dengeli ayır\n","train_balanced, test_balanced = train_test_split(\n","    balanced_df,\n","    test_size=0.2,\n","    stratify=balanced_df[\"label\"],\n","    random_state=42\n",")\n","\n","# 4) Oluşan eğitim ve test veri çerçevelerini (DataFrame) CSV formatında kaydet\n","train_balanced.to_csv(news_train_dataset_filepath, index=False)\n","test_balanced.to_csv(news_test_dataset_filepath, index=False)\n","\n","print(\"Eğitim kümesi boyutu:\", train_balanced.shape)\n","print(\"Test kümesi boyutu:\", test_balanced.shape)\n","print(\"\\nEğitim kümesindeki sınıfların dağılımı:\")\n","print(train_balanced[\"label\"].value_counts())\n","print(\"\\nTest kümesindeki sınıfların dağılımı:\")\n","print(test_balanced[\"label\"].value_counts())\n","print(\"\\nVeriler 'train_news.csv' ve 'test_news.csv' olarak kaydedildi.\\n\")\n","\n"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":221,"status":"ok","timestamp":1736596342816,"user":{"displayName":"Yusuf Enes Kurt","userId":"14520496146386168194"},"user_tz":-180},"id":"-ToQ4PhhwvFB","outputId":"3506c8a2-6749-45eb-dea2-69e886ab46ee"},"outputs":[{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 2000\n","    })\n","    test: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 500\n","    })\n","})\n"]}],"source":["from datasets import Dataset, DatasetDict\n","import pandas as pd\n","\n","directory = '/content/drive/MyDrive/Colab Notebooks/Kollektif_Ogrenme/Proje'\n","\n","# 5) CSV dosyalarından tekrar okuyarak Hugging Face Dataset'e dönüştürme\n","train_csv = pd.read_csv(directory+\"/news_dataset/train_news.csv\")\n","test_csv = pd.read_csv(directory+\"/news_dataset/test_news.csv\")\n","\n","train_dataset = Dataset.from_pandas(train_csv)\n","test_dataset = Dataset.from_pandas(test_csv)\n","\n","ds2 = DatasetDict({\n","    \"train\": train_dataset,\n","    \"test\": test_dataset\n","})\n","\n","print(ds2)"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNljwvq8/SF0T8xr86097wl"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}